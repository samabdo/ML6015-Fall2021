{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "089221f6",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>All-in-One project<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Define-a-problem-statement-and-goals\" data-toc-modified-id=\"Define-a-problem-statement-and-goals-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Define a problem statement and goals</a></span></li><li><span><a href=\"#Understand-the-dataset-and-features\" data-toc-modified-id=\"Understand-the-dataset-and-features-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span><a href=\"https://archive.ics.uci.edu/ml/datasets/hepatitis\" target=\"_blank\">Understand the dataset and features</a></a></span><ul class=\"toc-item\"><li><span><a href=\"#Features\" data-toc-modified-id=\"Features-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Features</a></span></li><li><span><a href=\"#Loading-the-dataset\" data-toc-modified-id=\"Loading-the-dataset-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Loading the dataset</a></span></li><li><span><a href=\"#Knowing-the-dataset-shape-and-features'-values-and-data-types\" data-toc-modified-id=\"Knowing-the-dataset-shape-and-features'-values-and-data-types-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Knowing the dataset shape and features' values and data types</a></span></li><li><span><a href=\"#Understanding-the-outcome-values:\" data-toc-modified-id=\"Understanding-the-outcome-values:-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Understanding the outcome values:</a></span></li></ul></li><li><span><a href=\"#Splitting-the-data:-Use-any-Splitting-Criterion\" data-toc-modified-id=\"Splitting-the-data:-Use-any-Splitting-Criterion-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Splitting the data: Use any Splitting Criterion</a></span></li><li><span><a href=\"#Data-preprocessing-and-algorithm-selection-steps-on-training/validation-then-use-the-output-parameters-from-the-training-on-testing\" data-toc-modified-id=\"Data-preprocessing-and-algorithm-selection-steps-on-training/validation-then-use-the-output-parameters-from-the-training-on-testing-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Data preprocessing and algorithm selection steps on training/validation then use the output parameters from the training on testing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Select-the-best-model-and-features\" data-toc-modified-id=\"Select-the-best-model-and-features-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Select the best model and features</a></span></li><li><span><a href=\"#How-do-you-comment-on-under-fitting-or-over-fitting?\" data-toc-modified-id=\"How-do-you-comment-on-under-fitting-or-over-fitting?-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>How do you comment on under-fitting or over-fitting?</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70549548",
   "metadata": {},
   "source": [
    "# Define a problem statement and goals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283406d6",
   "metadata": {},
   "source": [
    "# [Understand the dataset and features](https://archive.ics.uci.edu/ml/datasets/hepatitis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355c24f0",
   "metadata": {},
   "source": [
    "## Features\n",
    "   \n",
    "     1. Class: DIE, LIVE\n",
    "     2. AGE: 10, 20, 30, 40, 50, 60, 70, 80\n",
    "     3. SEX: male, female\n",
    "     4. STEROID: no, yes\n",
    "     5. ANTIVIRALS: no, yes\n",
    "     6. FATIGUE: no, yes\n",
    "     7. MALAISE: no, yes\n",
    "     8. ANOREXIA: no, yes\n",
    "     9. LIVER BIG: no, yes\n",
    "    10. LIVER FIRM: no, yes\n",
    "    11. SPLEEN PALPABLE: no, yes\n",
    "    12. SPIDERS: no, yes\n",
    "    13. ASCITES: no, yes\n",
    "    14. VARICES: no, yes\n",
    "    15. BILIRUBIN: 0.39, 0.80, 1.20, 2.00, 3.00, 4.00\n",
    "    16. ALK PHOSPHATE: 33, 80, 120, 160, 200, 250\n",
    "    17. SGOT: 13, 100, 200, 300, 400, 500, \n",
    "    18. ALBUMIN: 2.1, 3.0, 3.8, 4.5, 5.0, 6.0\n",
    "    19. PROTIME: 10, 20, 30, 40, 50, 60, 70, 80, 90\n",
    "    20. HISTOLOGY: no, yes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d43ee7",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16bf2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from scipy.stats import loguniform\n",
    "from statistics import mean\n",
    "from scipy import stats\n",
    "#import scikit_posthocs\n",
    "import warnings \n",
    "\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report,f1_score\n",
    "from sklearn.impute import SimpleImputer \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scikit_posthocs import posthoc_nemenyi_friedman\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a311bf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('~/DATA/hepatitis.csv', thousands=',', na_values='?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757c6985",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b030b8f",
   "metadata": {},
   "source": [
    "## Knowing the dataset shape and features' values and data types\n",
    "\n",
    "1. Is it classification/regression/unsupervised problem.\n",
    "2. You need to see if the dataset small and what are the acceptable values in features.\n",
    "3. Any decision in the above item should be considered in step 2 should be used in the external dataset. Example: If you remove noise/outliers from the whole dataset to clean dataset, keep track these decisions to apply in external examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2859d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b273b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba35632b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from the above there is no datatype object that condradicts with the above description\n",
    "allFeatures=data.columns[1:len(data.columns)]\n",
    "catFeatures=data.columns[list(range(2,14))+list(range(15,17))+list(range(18,20))]\n",
    "numFeatures= [i for i in allFeatures if not(i in catFeatures)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ff81bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "numFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5517f843",
   "metadata": {},
   "outputs": [],
   "source": [
    "catFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cbffa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#in-place command \n",
    "for c in catFeatures:\n",
    "    data[c]=data[c].astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a5a6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25bddc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[numFeatures].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14c2ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[numFeatures].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a03437",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [7.50, 3.50]\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "\n",
    "\n",
    "\n",
    "for cat in catFeatures:\n",
    "    fig, ax = plt.subplots()\n",
    "    data[cat].value_counts().plot(ax=ax, kind='bar', xlabel=cat, ylabel='frequency')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061cbd14",
   "metadata": {},
   "source": [
    "## Understanding the outcome values:\n",
    "1. If classification, see if you have imbalance values (equal ratios is balance). \n",
    "2. If regression, see if the outcome is skewed.\n",
    "3. I prefer not to do any step to deal with any outcome malfunction before splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e5ca2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imbalance dataset\n",
    "data['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7558f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[data.columns[1:20]]\n",
    "y = data[data.columns[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13c946a",
   "metadata": {},
   "source": [
    "# Splitting the data: Use any Splitting Criterion\n",
    "\n",
    "<b> In your project </b>\n",
    "\n",
    " 1. If the number of data points $\\le 1000$, use bootstrapping.\n",
    " 2. If you don't have enough data points with respect to the number of features, then use feature selection or reduction algorithm to reduce the number of features. You may use the rule of thumb like number of data points at least equals to 10/20 * number of features.\n",
    " 3. If you have very few number of features, then you may try to randomly select the data points to satisfy the Step 2 rule and repeat the process many times creating different models and average the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2d2d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since the size is less than 1000, we can use bootstapoing from the begining but here we will do 1 hoult out of 80% training and 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15b9786",
   "metadata": {},
   "source": [
    "# Data preprocessing and algorithm selection steps on training/validation then use the output parameters from the training on testing\n",
    "\n",
    "1. Removing Outliers/noise and conduct descriptive statistics if you don't do them before.\n",
    "2. Imputing the missing, balance the data by imbalance algorithms/Stratified Kfold, scaling numeric or encoding categorical features  and hyper-parameter optimization. \n",
    "3. Feature selection/dimensionality reduction methods; you may use them before or in-parallel with Step 4.\n",
    "4. If your problem is classification/regression, you can try clustering to understand better the problem.\n",
    "5. Select the algorithm(s):\n",
    "    - Do literature review to collect the methods that use.\n",
    "    - Determine what is the new in your method:\n",
    "        - Using new algorithm/dataset/features. -- in the project, you need to use at least new algorithm not taught in the course (Ask the instructor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd131c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check nulls in training\n",
    "X_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fc315a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#See https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html\n",
    "imp_mode = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "imp_mode.fit(X_train[catFeatures])\n",
    "imp_mean.fit(X_train[numFeatures])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38fc402",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[catFeatures]=imp_mode.transform(X_train[catFeatures])\n",
    "X_test[catFeatures]=imp_mode.transform(X_test[catFeatures])\n",
    "\n",
    "X_train[numFeatures]=imp_mean.transform(X_train[numFeatures])\n",
    "X_test[numFeatures]=imp_mean.transform(X_test[numFeatures])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b13b36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d5b80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387d1210",
   "metadata": {},
   "outputs": [],
   "source": [
    "scalor=StandardScaler()\n",
    "\n",
    "scalor.fit(X_train[numFeatures])\n",
    "X_train[numFeatures]=scalor.transform(X_train[numFeatures])\n",
    "X_test[numFeatures]=scalor.transform(X_test[numFeatures])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59137f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c29cc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fb0925",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=pd.get_dummies(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff448b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9023ef96",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=pd.get_dummies(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0a329f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f4d1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.reindex(columns = X_train.columns, fill_value=0)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef63b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de99af6",
   "metadata": {},
   "source": [
    "## Select the best model and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e169e773",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data is imbalance -- we can use imbalance algorithm in (https://pypi.org/project/imbalanced-learn/) or directly use repeatedstratisfied Kfold or both.\n",
    "\n",
    "rsFolds=RepeatedStratifiedKFold(n_splits=20, n_repeats=10, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4b32c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers=[LogisticRegression(), SVC(), KNeighborsClassifier()]\n",
    "\n",
    "#Logistic Regression parameter space\n",
    "parmLogi=dict()\n",
    "parmLogi['solver'] = [ 'liblinear']\n",
    "parmLogi['penalty'] = [ 'l1', 'l2']\n",
    "parmLogi['C'] = loguniform(1e-5, 100)\n",
    "\n",
    "#Support Vector Space parameter Space\n",
    "\n",
    "parm_SVCKernel = {'C': [0.1, 1, 10, 100, 1000], \n",
    "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "              'kernel': ['rbf']} \n",
    "\n",
    "\n",
    "#KNeighborsClassifier parameter Space\n",
    "max_K=int(math.sqrt(1500))\n",
    "parm_KNN= {\n",
    "    'n_neighbors':list(range(3,max_K,2)),\n",
    "    'metric':['euclidean','minkowski']\n",
    "}\n",
    "\n",
    "paramatersClassifiers=[parmLogi,parm_SVCKernel,parm_KNN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a05546",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the test to 100 splits of test data\n",
    "\n",
    "test_splits=[]\n",
    "\n",
    "for i in range(0,100):\n",
    "    S,S_y=resample(X_test,y_test,replace=True,random_state=0)\n",
    "    test_splits.append([S,S_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4711fa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "target_names = ['class 0', 'class 1']\n",
    "scoresTests=[]\n",
    "for i in range(0,3):\n",
    "    print('Classifier_'+str(i)+' --> ',classifiers[i])\n",
    "    model= RandomizedSearchCV(classifiers[i], paramatersClassifiers[i], scoring='f1_micro', cv=rsFolds, random_state=1)\n",
    "    model.fit(X_train,y_train)\n",
    "    print('Best Score: %s' % model.best_score_)\n",
    "    print('Best Hyperparameters: %s' % model.best_params_)\n",
    "    y_pred=model.predict(X_train)\n",
    "    print(classification_report(y_train, y_pred, target_names=target_names))\n",
    "    scorePerClassifier=[]\n",
    "    for j in range(0, len(test_splits)):\n",
    "        y_test_pred=model.predict(test_splits[j][0])\n",
    "        scorePerClassifier.append(f1_score(test_splits[j][1], y_test_pred, average='micro'))\n",
    "    print (mean(scorePerClassifier))\n",
    "    scoresTests.append(scorePerClassifier)                                                                                                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d9c834",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups=[scoresTests[i] for i in range(0,len(classifiers))]\n",
    "print(len(groups), len(groups[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1edbc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if Friedman test is signifiant\n",
    "\n",
    "chi_square,p_value_mean=stats.friedmanchisquare(*groups)\n",
    "print(p_value_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c906637b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If if Friedman test is signifiant, then do pairwise posthoc_nemenyi_friedman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1e9d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_groups=np.array(groups).T\n",
    "print(trans_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a07d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "p=posthoc_nemenyi_friedman(trans_groups)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1275c29c",
   "metadata": {},
   "source": [
    "## How do you comment on under-fitting or over-fitting?\n",
    "\n",
    "<b> From the above results , you can say the best is KNN, why?. </b>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "All-in-One project",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
