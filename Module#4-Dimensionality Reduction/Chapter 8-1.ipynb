{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Readings\n",
    "\n",
    "1. Chapter 8 in the syllabus book 1 covers the materials in this notebook and related notebooks. As guidelines for your readings, you may want to: \n",
    "    - Study first the notebooks and then read the related topics in Chapter 8 and web links for more knowledge. Or\n",
    "    - Read Chapter 8 to get broad vision and then study the notebooks and go through the web links when you need.\n",
    "2. Try to practice the code in Chapter 8 book 1 GiHub. \n",
    "3. You do not need to memorize the code but you can use/modify it in your assignment and project. I will not explain the codes in details; rather, I will just go through its main concepts and the outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copyright\n",
    "\n",
    "## Copyright holder\n",
    "\n",
    "Most of the contents of this lectures are taken/modified from the book:\n",
    "\"Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow Concepts, Tools, and Techniques to\n",
    "Build Intelligent Systems\"-- [link](https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1492032646). Other contents are either developed by the instructor or taken from web links that are cited in the contexts.      \n",
    "\n",
    "## The Course materials copyright\n",
    "\n",
    "The Content is made available only for your personal, noncommercial educational, and scholarly use. You may not use the Content for any other purpose, or distribute, post or make the Content available to others unless you obtain any required permission from the copyright holder. Some Content may be provided via streaming or other means that restrict copying; you may not circumvent those restrictions. You may not alter or remove any copyright or other proprietary notices included in the Content. You need to take the permission from the above copyright holder to use the materials and to check the complete Course Materials Copyright section in the course canvas website, as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The objectives\n",
    "1. Priniciple Component Analysis (PCA).\n",
    "2. Coveriance matrix.\n",
    "3. Eigenvectors and eigenvalues.\n",
    "4. The implementation of PCA steps,\n",
    "5. PCA in scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Dimenionality reduction][1]\n",
    "\n",
    "1. Feature elimination: We reduce the feature space by eliminating features--(Feature selction methods)\n",
    "2. Feature extraction: We create features from the data-- priniciple component analysis is one of them.\n",
    "\n",
    "## Priniciple Component Analysis (PCA)\n",
    "\n",
    "PCA is an unsupervised linear transformation technique. It combines our input variables in a specific way, then we can drop the “least important” variables while still retaining the most valuable parts of all of the variables! As an added benefit, each of the “new” variables after PCA are all independent of one another. \n",
    "\n",
    "## When should I use PCA?\n",
    "\n",
    "1. To conduct feature extraction and explaratory analysis.\n",
    "2. To reduce the number of predictors.\n",
    "2. To ensure the predictors are independent of one another.\n",
    "3. To be comfortable that they are less interpretable.\n",
    "\n",
    "## The main PCA cocnept:\n",
    "\n",
    "The orthogonal axes (principal components) of the new subspace can be interpreted as the directions of maximum variance given the constraint that the new feature axes are orthogonal to each other, as illustrated in the following figure:\n",
    "\n",
    "<img style=\"float:center\" src=\"./PCA concept.png\" alt=\"drawing\" height=\"200\" width=\"200\"/>\n",
    "\n",
    "\n",
    "## [PCA cores:][2]\n",
    "\n",
    "[Math. background](https://medium.com/data-science-365/statistical-and-mathematical-concepts-behind-pca-a2cb25940cd4)\n",
    "\n",
    "### 1. Coveriance matrix: \n",
    "- <b> Variance </b> is a measure of the variability or spread in a set of data: <br/> <br/>\n",
    "<center>$\\large \\sigma^2_{x}=\\frac{1}{n-1}\\sum_{i}^{n}(x_{i}-\\overline{x})^2$ </center> <br/>\n",
    "- <b> Covariance </b> is a measure of the extent to which corresponding elements from two sets of ordered data move in the same direction.<br/> <br/>\n",
    "<center>$\\large \\sigma(x,y)=\\frac{1}{n-1}\\sum_{i}^{n}(x_{i}-\\overline{x})(y_{i}-\\overline{y})$ </center> \n",
    "    - <b> Covariance Matrix: </b>  \n",
    "<center>$\\large C_{i,j}=\\sigma(x_{i},y_{j})$ where $C \\in \\mathbb{R}^{dxd}$ </center>\n",
    "\n",
    "### 2. Eigenvectors and eigenvalues:\n",
    "Eigenvectors are defined as those vectors whose directions remain unchanged after any linear transformation has been applied to them. However, their length could not remain the same after the transformation, i.e., the result of this transformation is the vector multiplied by a scalar. This scalar is called eigenvalue and each eigenvector has one associated to it. The number of eigenvectors or components that we can calculate for each data set is equal to the dimension of the data set. \n",
    " \n",
    " ### [3. Extracting eigenvectors and eigenvalues][3]\n",
    " \n",
    " Given the the dataset $Z$ and the coveriance matrix $C=Z^{T}Z$, we use [Singular Value Decomposition(SVD)][4] to decompose $C$ into $PDP^{-1}$ where $P$ is the matrix of eigenvectors and $D$ is the diagonal matrix with eigenvalues on the diagonal and values of zero everywhere else. The eigenvalues on the diagonal of $D$ will be associated with the corresponding column in $P$ — that is, the first element of $D$ is λ₁ and the corresponding eigenvector is the first column of $P$. \n",
    "\n",
    "[1]:https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c\n",
    "[2]:https://www.neuraldesigner.com/blog/principal-components-analysis\n",
    "[3]:https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c\n",
    "[4]:https://en.wikipedia.org/wiki/Singular_value_decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [PCA steps][1]\n",
    "\n",
    "\n",
    "\n",
    "1. Preparing the dataset\n",
    " - Loading the dataset\n",
    " - Exploratory visualization\n",
    "2. Standardizing the data\n",
    "3. Eigendecomposition - computing eigenvectors and eigenvalues\n",
    "  - Covariance Matrix\n",
    "  - Singular Value Decomposition to obtain eigenvectors and eigenvalues\n",
    "4. Selecting principal components\n",
    "  - Sorting eigenpairs\n",
    "  - Explained variance\n",
    "5. Projection matrix\n",
    "  - Projection onto the new feature space\n",
    "\n",
    "[1]:https://sebastianraschka.com/Articles/2015_pca_in_3_steps.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Example-- Let us practice][1]\n",
    "\n",
    "<b> Name of features: </b> Class,Alcohol, Malic acid, Ash, Alcalinity of ash,  Magnesium, Total phenols, Flavanoids,  Nonflavanoid phenols, Proanthocyanins, Color intensity, Hue, OD280/OD315 of diluted wines,Proline. Let us [practice][2]\n",
    "\n",
    "[1]:https://archive.ics.uci.edu/ml/machine-learning-databases/wine/\n",
    "[2]:https://dev.to/nexttech/principal-component-analysis-for-dimensionality-reduction-57i1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_wine = pd.read_csv('~/DATA/wine.csv',header=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting and standardize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# split into training and testing sets\n",
    "X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3,\n",
    "    stratify=y, random_state=0\n",
    ")\n",
    "# standardize the features\n",
    "sc = StandardScaler()\n",
    "X_train_std = sc.fit_transform(X_train)\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decompose  the convolution matrix into eigenvectors and eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "cov_mat = np.cov(X_train_std.T)\n",
    "eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)\n",
    "print(eigen_vecs)\n",
    "print(eigen_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the variance explained ratios and plot them\n",
    "\n",
    "The variance explained ratio of an eigenvalue λj is simply the fraction of an eigenvalue λj and the total sum of the eigenvalues: <br/>\n",
    "\n",
    "<center> $\\large \\frac{\\lambda_{j}}{\\sum_{j}^{d}\\lambda_{j}} $ </center>\n",
    "\n",
    "We can see that the first two principal components combined explain almost 60% of the variance in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# calculate cumulative sum of explained variances\n",
    "tot = sum(eigen_vals)\n",
    "var_exp = [(i / tot) for i in sorted(eigen_vals, reverse=True)]\n",
    "cum_var_exp = np.cumsum(var_exp)\n",
    "\n",
    "# plot explained variances\n",
    "plt.bar(range(1,14), var_exp, alpha=0.5,\n",
    "        align='center', label='individual explained variance')\n",
    "plt.step(range(1,14), cum_var_exp, where='mid',\n",
    "         label='cumulative explained variance')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xlabel('Principal component index')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort the eigenpairs by descending order of the eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of (eigenvalue, eigenvector) tuples\n",
    "eigen_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:, i]) for i in range(len(eigen_vals))]\n",
    "\n",
    "# Sort the (eigenvalue, eigenvector) tuples from high to low\n",
    "eigen_pairs.sort(key=lambda k: k[0], reverse=True)\n",
    "eigen_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct a projection matrix from the selected eigenvectors \n",
    "\n",
    "Using the two eigenvectors that correspond to the two largest eigenvalues, to capture about 60% of the variance in this dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.hstack((eigen_pairs[0][1][:, np.newaxis], eigen_pairs[1][1][:, np.newaxis]))\n",
    "print('Matrix W:\\n', w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data transformation\n",
    "\n",
    "We can transform the entire 124 x 13-dimensional training dataset onto the two principal components by calculating the matrix dot product:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca = X_train_std.dot(w)\n",
    "X_train_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the transformed Wine training set, now stored as an 124 x 2-dimensional matrix, in a two-dimensional scatterplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['r', 'b', 'g']\n",
    "markers = ['s', 'x', 'o']\n",
    "for l, c, m in zip(np.unique(y_train), colors, markers):\n",
    "    plt.scatter(X_train_pca[y_train==l, 0], \n",
    "                X_train_pca[y_train==l, 1], \n",
    "                c=c, label=l, marker=m) \n",
    "plt.xlabel('PC 1')\n",
    "plt.ylabel('PC 2')\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA in scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# intialize pca and logistic regression model\n",
    "pca = PCA(n_components=2)\n",
    "lr = LogisticRegression(multi_class='auto', solver='liblinear')\n",
    "\n",
    "# fit and transform data\n",
    "X_train_pca = pca.fit_transform(X_train_std)\n",
    "X_test_pca = pca.transform(X_test_std)\n",
    "lr.fit(X_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the decision regions on the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def plot_decision_regions(X, y, classifier, resolution=0.02):\n",
    "    # setup marker generator and color map\n",
    "    markers = ('s', 'x', 'o', '^', 'v')\n",
    "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "\n",
    "    # plot the decision surface\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
    "                           np.arange(x2_min, x2_max, resolution))\n",
    "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "\n",
    "    # plot class samples\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x=X[y == cl, 0], \n",
    "                    y=X[y == cl, 1],\n",
    "                    alpha=0.6, \n",
    "                    c=[cmap(idx)],\n",
    "                    edgecolor='black',\n",
    "                    marker=markers[idx], \n",
    "                    label=cl)# plot decision regions for training set\n",
    "\n",
    "\n",
    "plot_decision_regions(X_train_pca, y_train, classifier=lr)\n",
    "plt.xlabel('PC 1')\n",
    "plt.ylabel('PC 2')\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the decision regions of test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot decision regions for test set\n",
    "plot_decision_regions(X_test_pca, y_test, classifier=lr)\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Videos to watch\n",
    "\n",
    "1. [Covariance matrix][1]\n",
    "2. [Eigenvectors and Eigenvalues][2]\n",
    "3. [PCA][3]\n",
    "\n",
    "\n",
    "[1]:https://www.youtube.com/watch?v=0GzMcUy7ZI0\n",
    "[2]:http://videos.mathtutordvd.com/detail/videos/linear-algebra---vol-2/video/nddzsJAT21g/15---what-are-eigenvalues-and-eigenvectors-learn-how-to-find-eigenvalues.?autoStart=true\n",
    "[3]:https://www.youtube.com/watch?v=g-Hb26agBFg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
